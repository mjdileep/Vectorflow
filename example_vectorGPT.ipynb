{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d5b1e2-e93e-4855-9be0-c2d5207be8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectorflow.transformers import Transformer\n",
    "from vectorflow.optim import adam\n",
    "import numpy as np\n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2874100f-1b7e-4fa2-b789-9df2595b82ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs \n",
    "n_embed = 64 # Embedding dimensions \n",
    "block_size = 32 # Number timesteps (context size)\n",
    "keep=0.8  # Dropout keep p\n",
    "n_heads = 8 # Number of attention heads\n",
    "n_layers = 4 # Number of transfomer blocks \n",
    "batch_size = 64 # Training mini batch size\n",
    "max_iters = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc320acf-cab2-445b-bfd3-ca2e65edc5ea",
   "metadata": {},
   "source": [
    "## Create Char level token set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "068dd05f-3f03-4292-878e-37768b3591e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"input.txt\"):\n",
    "    !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "    \n",
    "with open(\"input.txt\") as fp:\n",
    "    text = fp.read()\n",
    "    \n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "\n",
    "# Prepare the dataset\n",
    "data = np.array(encode(text))\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = np.random.randint(len(data)-block_size-1, size=batch_size)\n",
    "    x = np.stack([data[i:i+block_size] for i in ix])\n",
    "    y = np.stack([data[i+1:i+1+block_size] for i in ix])\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5504bd54-785a-402c-b65e-a41420029c50",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cb536ba-9de8-4e48-99e5-771065bbbe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(vocab_size=vocab_size, n_embed= n_embed, \n",
    "                block_size= block_size, n_heads= n_heads, n_layers=n_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cf84f0-e0b2-49a0-b5a1-1d3e34bd29d8",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f745cbab-a5a5-48d3-9153-ea1cded4a793",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 5.0814, lr 0.001000\n",
      "Gradient Recieved: 1.9732398091420522\n",
      "Gradient after LM Head: 9.281999990455049\n",
      "Gradient after LN: 4.955359141658363\n",
      "Gradient after blocks: 10.719661283118416\n",
      "---------------------------------------\n",
      "\n",
      "step 10: train loss 3.6465, lr 0.001000\n",
      "Gradient Recieved: 1.899681453762146\n",
      "Gradient after LM Head: 8.72323696806592\n",
      "Gradient after LN: 3.327009389705029\n",
      "Gradient after blocks: 1.7244951579386765\n",
      "---------------------------------------\n",
      "\n",
      "step 20: train loss 3.4824, lr 0.001000\n",
      "Gradient Recieved: 1.884820275717595\n",
      "Gradient after LM Head: 8.647888712864605\n",
      "Gradient after LN: 2.724504633901182\n",
      "Gradient after blocks: 0.7407612413372773\n",
      "---------------------------------------\n",
      "\n",
      "step 30: train loss 3.4394, lr 0.001000\n",
      "Gradient Recieved: 1.8927781761713887\n",
      "Gradient after LM Head: 8.586134190530153\n",
      "Gradient after LN: 2.4886857047124487\n",
      "Gradient after blocks: 0.5445795929230445\n",
      "---------------------------------------\n",
      "\n",
      "step 40: train loss 3.3854, lr 0.001000\n",
      "Gradient Recieved: 1.8827742585782767\n",
      "Gradient after LM Head: 8.47419965907017\n",
      "Gradient after LN: 2.3729984438105434\n",
      "Gradient after blocks: 0.46868473335848687\n",
      "---------------------------------------\n",
      "\n",
      "step 50: train loss 3.4213, lr 0.001000\n",
      "Gradient Recieved: 1.8874532901013388\n",
      "Gradient after LM Head: 8.453564885440262\n",
      "Gradient after LN: 2.3257131465576406\n",
      "Gradient after blocks: 0.46081596154886173\n",
      "---------------------------------------\n",
      "\n",
      "step 60: train loss 3.4325, lr 0.001000\n",
      "Gradient Recieved: 1.8872032563084606\n",
      "Gradient after LM Head: 8.403725154046233\n",
      "Gradient after LN: 2.3093285868971463\n",
      "Gradient after blocks: 0.48337971403081825\n",
      "---------------------------------------\n",
      "\n",
      "step 70: train loss 3.4628, lr 0.001000\n",
      "Gradient Recieved: 1.8938049687850869\n",
      "Gradient after LM Head: 8.368414864203299\n",
      "Gradient after LN: 2.3069774750370184\n",
      "Gradient after blocks: 0.5118698326608302\n",
      "---------------------------------------\n",
      "\n",
      "step 80: train loss 3.4214, lr 0.001000\n",
      "Gradient Recieved: 1.8864146750889965\n",
      "Gradient after LM Head: 8.281034238795016\n",
      "Gradient after LN: 2.2950238066159905\n",
      "Gradient after blocks: 0.5500409503117851\n",
      "---------------------------------------\n",
      "\n",
      "step 90: train loss 3.3932, lr 0.001000\n",
      "Gradient Recieved: 1.8877004463876452\n",
      "Gradient after LM Head: 8.2542579271766\n",
      "Gradient after LN: 2.3112088053030515\n",
      "Gradient after blocks: 0.6022830042911268\n",
      "---------------------------------------\n",
      "\n",
      "step 100: train loss 3.3772, lr 0.001000\n",
      "Gradient Recieved: 1.8870102773663755\n",
      "Gradient after LM Head: 8.194664885991223\n",
      "Gradient after LN: 2.320957112707299\n",
      "Gradient after blocks: 0.6756276270358152\n",
      "---------------------------------------\n",
      "\n",
      "step 110: train loss 3.3636, lr 0.001000\n",
      "Gradient Recieved: 1.8840510299411704\n",
      "Gradient after LM Head: 8.117552214337731\n",
      "Gradient after LN: 2.3221808250056784\n",
      "Gradient after blocks: 0.8439569350581734\n",
      "---------------------------------------\n",
      "\n",
      "step 120: train loss 3.4349, lr 0.001000\n",
      "Gradient Recieved: 1.8913880306806945\n",
      "Gradient after LM Head: 8.119177269813202\n",
      "Gradient after LN: 2.3783952027439046\n",
      "Gradient after blocks: 1.069753283224031\n",
      "---------------------------------------\n",
      "\n",
      "step 130: train loss 3.3617, lr 0.001000\n",
      "Gradient Recieved: 1.8854629471790547\n",
      "Gradient after LM Head: 8.050989535478143\n",
      "Gradient after LN: 2.3977674782844\n",
      "Gradient after blocks: 1.4731839200954109\n",
      "---------------------------------------\n",
      "\n",
      "step 140: train loss 3.4227, lr 0.001000\n",
      "Gradient Recieved: 1.8889279100960568\n",
      "Gradient after LM Head: 8.030830336375944\n",
      "Gradient after LN: 2.423850543867415\n",
      "Gradient after blocks: 2.2219311242180955\n",
      "---------------------------------------\n",
      "\n",
      "step 150: train loss 3.3541, lr 0.001000\n",
      "Gradient Recieved: 1.8797192737551678\n",
      "Gradient after LM Head: 7.948958988125761\n",
      "Gradient after LN: 2.502582845785298\n",
      "Gradient after blocks: 3.6434449551761774\n",
      "---------------------------------------\n",
      "\n",
      "step 160: train loss 3.4328, lr 0.001000\n",
      "Gradient Recieved: 1.8730620139980445\n",
      "Gradient after LM Head: 7.9339557542343595\n",
      "Gradient after LN: 2.635275589203303\n",
      "Gradient after blocks: 6.5083639024823965\n",
      "---------------------------------------\n",
      "\n",
      "step 170: train loss 3.2174, lr 0.001000\n",
      "Gradient Recieved: 1.8495097128145281\n",
      "Gradient after LM Head: 7.76708605525904\n",
      "Gradient after LN: 2.6909278612318888\n",
      "Gradient after blocks: 8.930528958570441\n",
      "---------------------------------------\n",
      "\n",
      "step 180: train loss 3.2227, lr 0.001000\n",
      "Gradient Recieved: 1.8498247271518877\n",
      "Gradient after LM Head: 7.743819714561365\n",
      "Gradient after LN: 2.545814902100629\n",
      "Gradient after blocks: 8.151156945349\n",
      "---------------------------------------\n",
      "\n",
      "step 190: train loss 3.1244, lr 0.001000\n",
      "Gradient Recieved: 1.8251205799208243\n",
      "Gradient after LM Head: 7.616634785276026\n",
      "Gradient after LN: 2.572165616895959\n",
      "Gradient after blocks: 10.294724076696259\n",
      "---------------------------------------\n",
      "\n",
      "step 200: train loss 3.1310, lr 0.001000\n",
      "Gradient Recieved: 1.8312709063372485\n",
      "Gradient after LM Head: 7.6285098587158\n",
      "Gradient after LN: 2.3580059503224877\n",
      "Gradient after blocks: 9.166575017891969\n",
      "---------------------------------------\n",
      "\n",
      "step 210: train loss 3.0411, lr 0.001000\n",
      "Gradient Recieved: 1.808536769056101\n",
      "Gradient after LM Head: 7.509450400280113\n",
      "Gradient after LN: 2.2977891131576924\n",
      "Gradient after blocks: 9.748264592033902\n",
      "---------------------------------------\n",
      "\n",
      "step 220: train loss 2.9372, lr 0.001000\n",
      "Gradient Recieved: 1.7925461220365078\n",
      "Gradient after LM Head: 7.440359044428417\n",
      "Gradient after LN: 2.1391419208417712\n",
      "Gradient after blocks: 8.777612263578968\n",
      "---------------------------------------\n",
      "\n",
      "step 230: train loss 2.9886, lr 0.001000\n",
      "Gradient Recieved: 1.797312233787752\n",
      "Gradient after LM Head: 7.401812556096162\n",
      "Gradient after LN: 2.1441221763605856\n",
      "Gradient after blocks: 9.133672999079158\n",
      "---------------------------------------\n",
      "\n",
      "step 240: train loss 2.9404, lr 0.001000\n",
      "Gradient Recieved: 1.7829738271382558\n",
      "Gradient after LM Head: 7.366238593419929\n",
      "Gradient after LN: 2.0943361929420194\n",
      "Gradient after blocks: 8.818069603557873\n",
      "---------------------------------------\n",
      "\n",
      "step 250: train loss 2.9545, lr 0.001000\n",
      "Gradient Recieved: 1.782122328552084\n",
      "Gradient after LM Head: 7.3361700478110095\n",
      "Gradient after LN: 2.0879782073617954\n",
      "Gradient after blocks: 9.169463085991211\n",
      "---------------------------------------\n",
      "\n",
      "step 260: train loss 2.8625, lr 0.001000\n",
      "Gradient Recieved: 1.7733511291253392\n",
      "Gradient after LM Head: 7.232977337933953\n",
      "Gradient after LN: 2.0002761894093797\n",
      "Gradient after blocks: 8.775221370375522\n",
      "---------------------------------------\n",
      "\n",
      "step 270: train loss 2.8369, lr 0.001000\n",
      "Gradient Recieved: 1.7668927510537875\n",
      "Gradient after LM Head: 7.183409045502193\n",
      "Gradient after LN: 1.9415486198918723\n",
      "Gradient after blocks: 8.36848582804072\n",
      "---------------------------------------\n",
      "\n",
      "step 280: train loss 2.8806, lr 0.001000\n",
      "Gradient Recieved: 1.7731844037781381\n",
      "Gradient after LM Head: 7.178123121113864\n",
      "Gradient after LN: 1.9298461494966643\n",
      "Gradient after blocks: 8.751718288772988\n",
      "---------------------------------------\n",
      "\n",
      "step 290: train loss 2.8210, lr 0.001000\n",
      "Gradient Recieved: 1.7680085395493808\n",
      "Gradient after LM Head: 7.13740712153337\n",
      "Gradient after LN: 1.8812271708323904\n",
      "Gradient after blocks: 8.875033976500706\n",
      "---------------------------------------\n",
      "\n",
      "step 300: train loss 2.7991, lr 0.001000\n",
      "Gradient Recieved: 1.7546426549633776\n",
      "Gradient after LM Head: 7.075161299001777\n",
      "Gradient after LN: 1.8885250928759127\n",
      "Gradient after blocks: 9.229529519060494\n",
      "---------------------------------------\n",
      "\n",
      "step 310: train loss 2.8098, lr 0.001000\n",
      "Gradient Recieved: 1.7577733256745756\n",
      "Gradient after LM Head: 7.04205964910772\n",
      "Gradient after LN: 1.8228409829507084\n",
      "Gradient after blocks: 9.097660948002876\n",
      "---------------------------------------\n",
      "\n",
      "step 320: train loss 2.7686, lr 0.001000\n",
      "Gradient Recieved: 1.74932745208939\n",
      "Gradient after LM Head: 6.981360902908012\n",
      "Gradient after LN: 1.7938631247080854\n",
      "Gradient after blocks: 9.265047587130153\n",
      "---------------------------------------\n",
      "\n",
      "step 330: train loss 2.7534, lr 0.001000\n",
      "Gradient Recieved: 1.7565484052010865\n",
      "Gradient after LM Head: 6.972081259472128\n",
      "Gradient after LN: 1.763578013482634\n",
      "Gradient after blocks: 9.359986191242907\n",
      "---------------------------------------\n",
      "\n",
      "step 340: train loss 2.7290, lr 0.001000\n",
      "Gradient Recieved: 1.7474453058456674\n",
      "Gradient after LM Head: 6.92034475762593\n",
      "Gradient after LN: 1.72308476065183\n",
      "Gradient after blocks: 9.224521958752309\n",
      "---------------------------------------\n",
      "\n",
      "step 350: train loss 2.7572, lr 0.001000\n",
      "Gradient Recieved: 1.7476821848379398\n",
      "Gradient after LM Head: 6.920078762159108\n",
      "Gradient after LN: 1.68033025664753\n",
      "Gradient after blocks: 9.255991852043993\n",
      "---------------------------------------\n",
      "\n",
      "step 360: train loss 2.7559, lr 0.001000\n",
      "Gradient Recieved: 1.7531566636906553\n",
      "Gradient after LM Head: 6.896809230304955\n",
      "Gradient after LN: 1.6525338518719033\n",
      "Gradient after blocks: 9.117879565071666\n",
      "---------------------------------------\n",
      "\n",
      "step 370: train loss 2.7184, lr 0.001000\n",
      "Gradient Recieved: 1.7431631320799992\n",
      "Gradient after LM Head: 6.8421673051155825\n",
      "Gradient after LN: 1.6160382261108566\n",
      "Gradient after blocks: 9.517623507315733\n",
      "---------------------------------------\n",
      "\n",
      "step 380: train loss 2.7310, lr 0.001000\n",
      "Gradient Recieved: 1.7457453757246142\n",
      "Gradient after LM Head: 6.813983404083362\n",
      "Gradient after LN: 1.5701061514830679\n",
      "Gradient after blocks: 9.614046398081074\n",
      "---------------------------------------\n",
      "\n",
      "step 390: train loss 2.6973, lr 0.001000\n",
      "Gradient Recieved: 1.7428819101243098\n",
      "Gradient after LM Head: 6.770278465480878\n",
      "Gradient after LN: 1.5723473370304872\n",
      "Gradient after blocks: 9.052385561737266\n",
      "---------------------------------------\n",
      "\n",
      "step 400: train loss 2.7359, lr 0.001000\n",
      "Gradient Recieved: 1.739852663732598\n",
      "Gradient after LM Head: 6.788316529265481\n",
      "Gradient after LN: 1.5297277802571827\n",
      "Gradient after blocks: 9.256624059564848\n",
      "---------------------------------------\n",
      "\n",
      "step 410: train loss 2.6573, lr 0.001000\n",
      "Gradient Recieved: 1.728707613247472\n",
      "Gradient after LM Head: 6.710306124634241\n",
      "Gradient after LN: 1.5095901219779102\n",
      "Gradient after blocks: 8.740761852608408\n",
      "---------------------------------------\n",
      "\n",
      "step 420: train loss 2.7064, lr 0.001000\n",
      "Gradient Recieved: 1.7435151920289271\n",
      "Gradient after LM Head: 6.780995245798565\n",
      "Gradient after LN: 1.5342100707036213\n",
      "Gradient after blocks: 9.29554600836089\n",
      "---------------------------------------\n",
      "\n",
      "step 430: train loss 2.7362, lr 0.001000\n",
      "Gradient Recieved: 1.7450378086397154\n",
      "Gradient after LM Head: 6.790026104914217\n",
      "Gradient after LN: 1.4895959849167546\n",
      "Gradient after blocks: 9.092942132456022\n",
      "---------------------------------------\n",
      "\n",
      "step 440: train loss 2.7208, lr 0.001000\n",
      "Gradient Recieved: 1.7434851055426421\n",
      "Gradient after LM Head: 6.725037817917038\n",
      "Gradient after LN: 1.4773287818434584\n",
      "Gradient after blocks: 8.866510161524177\n",
      "---------------------------------------\n",
      "\n",
      "step 450: train loss 2.7016, lr 0.001000\n",
      "Gradient Recieved: 1.7369687863011394\n",
      "Gradient after LM Head: 6.742819712598486\n",
      "Gradient after LN: 1.5040142476080964\n",
      "Gradient after blocks: 8.973135570754481\n",
      "---------------------------------------\n",
      "\n",
      "step 460: train loss 2.7155, lr 0.001000\n",
      "Gradient Recieved: 1.7340067000340371\n",
      "Gradient after LM Head: 6.694967301766148\n",
      "Gradient after LN: 1.485088834280596\n",
      "Gradient after blocks: 9.2261531720597\n",
      "---------------------------------------\n",
      "\n",
      "step 470: train loss 2.6644, lr 0.001000\n",
      "Gradient Recieved: 1.7292583306235623\n",
      "Gradient after LM Head: 6.647543677877733\n",
      "Gradient after LN: 1.4678990373988905\n",
      "Gradient after blocks: 9.278666975335733\n",
      "---------------------------------------\n",
      "\n",
      "step 480: train loss 2.6807, lr 0.001000\n",
      "Gradient Recieved: 1.7277834414102033\n",
      "Gradient after LM Head: 6.636695750613299\n",
      "Gradient after LN: 1.458821699582155\n",
      "Gradient after blocks: 8.742954018818489\n",
      "---------------------------------------\n",
      "\n",
      "step 490: train loss 2.7006, lr 0.001000\n",
      "Gradient Recieved: 1.7319960079501215\n",
      "Gradient after LM Head: 6.632317181680248\n",
      "Gradient after LN: 1.4491052857956104\n",
      "Gradient after blocks: 8.786844568476798\n",
      "---------------------------------------\n",
      "\n",
      "step 500: train loss 2.6834, lr 0.001000\n",
      "Gradient Recieved: 1.7358807177862656\n",
      "Gradient after LM Head: 6.649963545716655\n",
      "Gradient after LN: 1.4519276704953636\n",
      "Gradient after blocks: 9.093694307497898\n",
      "---------------------------------------\n",
      "\n",
      "step 510: train loss 2.6965, lr 0.001000\n",
      "Gradient Recieved: 1.7305447612670442\n",
      "Gradient after LM Head: 6.655368793527192\n",
      "Gradient after LN: 1.4734440897589587\n",
      "Gradient after blocks: 8.997910462949205\n",
      "---------------------------------------\n",
      "\n",
      "step 520: train loss 2.7077, lr 0.001000\n",
      "Gradient Recieved: 1.7160727608091118\n",
      "Gradient after LM Head: 6.604405795574733\n",
      "Gradient after LN: 1.4420939147728913\n",
      "Gradient after blocks: 8.970776374712404\n",
      "---------------------------------------\n",
      "\n",
      "step 530: train loss 2.6869, lr 0.001000\n",
      "Gradient Recieved: 1.7239235944112283\n",
      "Gradient after LM Head: 6.634588592744257\n",
      "Gradient after LN: 1.4636631031672127\n",
      "Gradient after blocks: 9.057550936452174\n",
      "---------------------------------------\n",
      "\n",
      "step 540: train loss 2.7176, lr 0.001000\n",
      "Gradient Recieved: 1.7354857718719956\n",
      "Gradient after LM Head: 6.63865500216612\n",
      "Gradient after LN: 1.4391031939389942\n",
      "Gradient after blocks: 9.222330245205475\n",
      "---------------------------------------\n",
      "\n",
      "step 550: train loss 2.6688, lr 0.001000\n",
      "Gradient Recieved: 1.7380389168580477\n",
      "Gradient after LM Head: 6.577533866205635\n",
      "Gradient after LN: 1.3860414174963516\n",
      "Gradient after blocks: 8.430865666415038\n",
      "---------------------------------------\n",
      "\n",
      "step 560: train loss 2.6140, lr 0.001000\n",
      "Gradient Recieved: 1.7292548979381817\n",
      "Gradient after LM Head: 6.4954030869951245\n",
      "Gradient after LN: 1.3555878550702087\n",
      "Gradient after blocks: 8.43628842178425\n",
      "---------------------------------------\n",
      "\n",
      "step 570: train loss 2.6999, lr 0.001000\n",
      "Gradient Recieved: 1.7285725876098939\n",
      "Gradient after LM Head: 6.62317155863473\n",
      "Gradient after LN: 1.4172028047983096\n",
      "Gradient after blocks: 9.177105867118465\n",
      "---------------------------------------\n",
      "\n",
      "step 580: train loss 2.6517, lr 0.001000\n",
      "Gradient Recieved: 1.7275301052857497\n",
      "Gradient after LM Head: 6.500498521017576\n",
      "Gradient after LN: 1.3763803225646163\n",
      "Gradient after blocks: 8.724218842965211\n",
      "---------------------------------------\n",
      "\n",
      "step 590: train loss 2.6357, lr 0.001000\n",
      "Gradient Recieved: 1.732428702098995\n",
      "Gradient after LM Head: 6.543267515455946\n",
      "Gradient after LN: 1.3587456251474204\n",
      "Gradient after blocks: 8.529279102395032\n",
      "---------------------------------------\n",
      "\n",
      "step 600: train loss 2.6237, lr 0.001000\n",
      "Gradient Recieved: 1.72322289792489\n",
      "Gradient after LM Head: 6.52971131130703\n",
      "Gradient after LN: 1.3866973816198538\n",
      "Gradient after blocks: 8.88519985027674\n",
      "---------------------------------------\n",
      "\n",
      "step 610: train loss 2.6574, lr 0.001000\n",
      "Gradient Recieved: 1.7317618265760046\n",
      "Gradient after LM Head: 6.559993980639446\n",
      "Gradient after LN: 1.3922357583283447\n",
      "Gradient after blocks: 8.536458342018392\n",
      "---------------------------------------\n",
      "\n",
      "step 620: train loss 2.6665, lr 0.001000\n",
      "Gradient Recieved: 1.720822091092652\n",
      "Gradient after LM Head: 6.546649751022171\n",
      "Gradient after LN: 1.438476196516618\n",
      "Gradient after blocks: 8.773233726305977\n",
      "---------------------------------------\n",
      "\n",
      "step 630: train loss 2.6515, lr 0.001000\n",
      "Gradient Recieved: 1.7353750643446104\n",
      "Gradient after LM Head: 6.50673850594095\n",
      "Gradient after LN: 1.3247510588907594\n",
      "Gradient after blocks: 8.133900977502856\n",
      "---------------------------------------\n",
      "\n",
      "step 640: train loss 2.6370, lr 0.001000\n",
      "Gradient Recieved: 1.7351363384728418\n",
      "Gradient after LM Head: 6.47156704870957\n",
      "Gradient after LN: 1.3299993075396739\n",
      "Gradient after blocks: 8.339134679146364\n",
      "---------------------------------------\n",
      "\n",
      "step 650: train loss 2.6371, lr 0.001000\n",
      "Gradient Recieved: 1.7170948901746406\n",
      "Gradient after LM Head: 6.4492713221479585\n",
      "Gradient after LN: 1.3353782506083032\n",
      "Gradient after blocks: 8.200676991640307\n",
      "---------------------------------------\n",
      "\n",
      "step 660: train loss 2.6234, lr 0.001000\n",
      "Gradient Recieved: 1.7238817312793486\n",
      "Gradient after LM Head: 6.454382668919978\n",
      "Gradient after LN: 1.33480249403708\n",
      "Gradient after blocks: 8.073871144450193\n",
      "---------------------------------------\n",
      "\n",
      "step 670: train loss 2.6854, lr 0.001000\n",
      "Gradient Recieved: 1.7323732359039248\n",
      "Gradient after LM Head: 6.495778129239015\n",
      "Gradient after LN: 1.3705343269828236\n",
      "Gradient after blocks: 8.508643427342308\n",
      "---------------------------------------\n",
      "\n",
      "step 680: train loss 2.6393, lr 0.001000\n",
      "Gradient Recieved: 1.732920672262771\n",
      "Gradient after LM Head: 6.489068098494056\n",
      "Gradient after LN: 1.3435372431909958\n",
      "Gradient after blocks: 8.415262355847199\n",
      "---------------------------------------\n",
      "\n",
      "step 690: train loss 2.6265, lr 0.001000\n",
      "Gradient Recieved: 1.7296334877512773\n",
      "Gradient after LM Head: 6.462464902530734\n",
      "Gradient after LN: 1.318624950582684\n",
      "Gradient after blocks: 8.399444732908607\n",
      "---------------------------------------\n",
      "\n",
      "step 700: train loss 2.6476, lr 0.001000\n",
      "Gradient Recieved: 1.7211873857714173\n",
      "Gradient after LM Head: 6.429260007783075\n",
      "Gradient after LN: 1.3083972988735664\n",
      "Gradient after blocks: 8.351124593967521\n",
      "---------------------------------------\n",
      "\n",
      "step 710: train loss 2.6385, lr 0.001000\n",
      "Gradient Recieved: 1.730503024402401\n",
      "Gradient after LM Head: 6.451166583741716\n",
      "Gradient after LN: 1.3110074672727265\n",
      "Gradient after blocks: 8.545606840669242\n",
      "---------------------------------------\n",
      "\n",
      "step 720: train loss 2.6192, lr 0.001000\n",
      "Gradient Recieved: 1.7195437741906818\n",
      "Gradient after LM Head: 6.386698517819487\n",
      "Gradient after LN: 1.2956817491448673\n",
      "Gradient after blocks: 8.098648194239964\n",
      "---------------------------------------\n",
      "\n",
      "step 730: train loss 2.6379, lr 0.001000\n",
      "Gradient Recieved: 1.725921478473871\n",
      "Gradient after LM Head: 6.444065842326161\n",
      "Gradient after LN: 1.3220177787475271\n",
      "Gradient after blocks: 8.419260288299876\n",
      "---------------------------------------\n",
      "\n",
      "step 740: train loss 2.6537, lr 0.001000\n",
      "Gradient Recieved: 1.721156856289218\n",
      "Gradient after LM Head: 6.414041002971833\n",
      "Gradient after LN: 1.3527117072673562\n",
      "Gradient after blocks: 8.471070155566748\n",
      "---------------------------------------\n",
      "\n",
      "step 750: train loss 2.6197, lr 0.001000\n",
      "Gradient Recieved: 1.729422339539179\n",
      "Gradient after LM Head: 6.385377550868167\n",
      "Gradient after LN: 1.3209949970997779\n",
      "Gradient after blocks: 8.284615374548341\n",
      "---------------------------------------\n",
      "\n",
      "step 760: train loss 2.5869, lr 0.001000\n",
      "Gradient Recieved: 1.719352269598768\n",
      "Gradient after LM Head: 6.3516989389324445\n",
      "Gradient after LN: 1.3012942583353149\n",
      "Gradient after blocks: 8.115800129411143\n",
      "---------------------------------------\n",
      "\n",
      "step 770: train loss 2.6416, lr 0.001000\n",
      "Gradient Recieved: 1.7270704251223887\n",
      "Gradient after LM Head: 6.396112432968845\n",
      "Gradient after LN: 1.316295400673373\n",
      "Gradient after blocks: 8.477986805601176\n",
      "---------------------------------------\n",
      "\n",
      "step 780: train loss 2.6272, lr 0.001000\n",
      "Gradient Recieved: 1.726900629894585\n",
      "Gradient after LM Head: 6.428333748990286\n",
      "Gradient after LN: 1.3301432518550875\n",
      "Gradient after blocks: 8.460471814212644\n",
      "---------------------------------------\n",
      "\n",
      "step 790: train loss 2.5851, lr 0.001000\n",
      "Gradient Recieved: 1.7145842911105125\n",
      "Gradient after LM Head: 6.356984939679633\n",
      "Gradient after LN: 1.2936931945114891\n",
      "Gradient after blocks: 8.128825806466283\n",
      "---------------------------------------\n",
      "\n",
      "step 800: train loss 2.6616, lr 0.001000\n",
      "Gradient Recieved: 1.7210970943290234\n",
      "Gradient after LM Head: 6.4205339704370346\n",
      "Gradient after LN: 1.3400519816034044\n",
      "Gradient after blocks: 8.704738660627964\n",
      "---------------------------------------\n",
      "\n",
      "step 810: train loss 2.6268, lr 0.001000\n",
      "Gradient Recieved: 1.7154939494560346\n",
      "Gradient after LM Head: 6.384388433064872\n",
      "Gradient after LN: 1.3186723339777207\n",
      "Gradient after blocks: 8.433332408106706\n",
      "---------------------------------------\n",
      "\n",
      "step 820: train loss 2.5932, lr 0.001000\n",
      "Gradient Recieved: 1.7209943441122457\n",
      "Gradient after LM Head: 6.339567470225971\n",
      "Gradient after LN: 1.3091821747026047\n",
      "Gradient after blocks: 8.112277239817175\n",
      "---------------------------------------\n",
      "\n",
      "step 830: train loss 2.6249, lr 0.001000\n",
      "Gradient Recieved: 1.7258675337453953\n",
      "Gradient after LM Head: 6.40121310354625\n",
      "Gradient after LN: 1.3504842479019665\n",
      "Gradient after blocks: 8.453112501018776\n",
      "---------------------------------------\n",
      "\n",
      "step 840: train loss 2.5821, lr 0.001000\n",
      "Gradient Recieved: 1.7170956049206492\n",
      "Gradient after LM Head: 6.34925368470429\n",
      "Gradient after LN: 1.320989649044342\n",
      "Gradient after blocks: 8.136042281824698\n",
      "---------------------------------------\n",
      "\n",
      "step 850: train loss 2.6335, lr 0.001000\n",
      "Gradient Recieved: 1.7240930134803336\n",
      "Gradient after LM Head: 6.376240524750441\n",
      "Gradient after LN: 1.3294169629648067\n",
      "Gradient after blocks: 8.424293773933677\n",
      "---------------------------------------\n",
      "\n",
      "step 860: train loss 2.6111, lr 0.001000\n",
      "Gradient Recieved: 1.7268123898394854\n",
      "Gradient after LM Head: 6.382224219049847\n",
      "Gradient after LN: 1.3409700698914202\n",
      "Gradient after blocks: 8.106436127077611\n",
      "---------------------------------------\n",
      "\n",
      "step 870: train loss 2.6135, lr 0.001000\n",
      "Gradient Recieved: 1.717634518386537\n",
      "Gradient after LM Head: 6.350225685025873\n",
      "Gradient after LN: 1.3280421502345137\n",
      "Gradient after blocks: 8.257581631345623\n",
      "---------------------------------------\n",
      "\n",
      "step 880: train loss 2.6182, lr 0.001000\n",
      "Gradient Recieved: 1.7242104641930505\n",
      "Gradient after LM Head: 6.340882286873409\n",
      "Gradient after LN: 1.3150824085315753\n",
      "Gradient after blocks: 8.177638838001098\n",
      "---------------------------------------\n",
      "\n",
      "step 890: train loss 2.5446, lr 0.001000\n",
      "Gradient Recieved: 1.7088319540415566\n",
      "Gradient after LM Head: 6.2664496809434445\n",
      "Gradient after LN: 1.3077011021642575\n",
      "Gradient after blocks: 7.8490216340954095\n",
      "---------------------------------------\n",
      "\n",
      "step 900: train loss 2.5920, lr 0.001000\n",
      "Gradient Recieved: 1.7234350543059376\n",
      "Gradient after LM Head: 6.306379397201129\n",
      "Gradient after LN: 1.2908046838920901\n",
      "Gradient after blocks: 7.983312568290673\n",
      "---------------------------------------\n",
      "\n",
      "step 910: train loss 2.5665, lr 0.001000\n",
      "Gradient Recieved: 1.7198509320299722\n",
      "Gradient after LM Head: 6.323512610442799\n",
      "Gradient after LN: 1.2926199548267916\n",
      "Gradient after blocks: 7.640277446614305\n",
      "---------------------------------------\n",
      "\n",
      "step 920: train loss 2.5897, lr 0.001000\n",
      "Gradient Recieved: 1.7090867060768566\n",
      "Gradient after LM Head: 6.27584327972872\n",
      "Gradient after LN: 1.315753489403883\n",
      "Gradient after blocks: 8.112870860816935\n",
      "---------------------------------------\n",
      "\n",
      "step 930: train loss 2.6000, lr 0.001000\n",
      "Gradient Recieved: 1.7110340307654397\n",
      "Gradient after LM Head: 6.322974377599098\n",
      "Gradient after LN: 1.3527916346744817\n",
      "Gradient after blocks: 8.099393184018462\n",
      "---------------------------------------\n",
      "\n",
      "step 940: train loss 2.6133, lr 0.001000\n",
      "Gradient Recieved: 1.7179452143466727\n",
      "Gradient after LM Head: 6.292080681232065\n",
      "Gradient after LN: 1.309599002545782\n",
      "Gradient after blocks: 7.8742703320635465\n",
      "---------------------------------------\n",
      "\n",
      "step 950: train loss 2.5761, lr 0.001000\n",
      "Gradient Recieved: 1.7114840265893891\n",
      "Gradient after LM Head: 6.238129368374837\n",
      "Gradient after LN: 1.2873082363153516\n",
      "Gradient after blocks: 7.794621531207417\n",
      "---------------------------------------\n",
      "\n",
      "step 960: train loss 2.5705, lr 0.001000\n",
      "Gradient Recieved: 1.709057960482527\n",
      "Gradient after LM Head: 6.295587996994158\n",
      "Gradient after LN: 1.3100845825495113\n",
      "Gradient after blocks: 7.954656857029501\n",
      "---------------------------------------\n",
      "\n",
      "step 970: train loss 2.5710, lr 0.001000\n",
      "Gradient Recieved: 1.7163079164749746\n",
      "Gradient after LM Head: 6.269298958299643\n",
      "Gradient after LN: 1.3040582216059338\n",
      "Gradient after blocks: 7.836276592013024\n",
      "---------------------------------------\n",
      "\n",
      "step 980: train loss 2.6011, lr 0.001000\n",
      "Gradient Recieved: 1.715255915250537\n",
      "Gradient after LM Head: 6.297532780872556\n",
      "Gradient after LN: 1.3224258610620085\n",
      "Gradient after blocks: 7.9844381404580815\n",
      "---------------------------------------\n",
      "\n",
      "step 990: train loss 2.6156, lr 0.001000\n",
      "Gradient Recieved: 1.7216652139764328\n",
      "Gradient after LM Head: 6.3369390582070375\n",
      "Gradient after LN: 1.3739045510227386\n",
      "Gradient after blocks: 8.052493973934078\n",
      "---------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-3\n",
    "last_loss = 1000\n",
    "for iter in range(max_iters):\n",
    "    xb, yb = get_batch(\"train\")\n",
    "    logits, loss, dout = model(xb, train=True, targets=yb)\n",
    "    print_grad = False\n",
    "    if iter % 10 ==0:\n",
    "        print(f\"step {iter}: train loss {loss:.4f}, lr {learning_rate:.6f}\")\n",
    "        print_grad = True\n",
    "    model.grad_zero()\n",
    "    model.backward(dout, print_grad=print_grad)\n",
    "    model.step(adam, learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7e9e59-4699-4b45-802c-cfb341d754f8",
   "metadata": {},
   "source": [
    "## Sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08d1dadd-1edf-4eaa-83c9-6ea7f1c56994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " d s and I hithice s be aree the s fore s o and ce th be ar cou the be aghin this En ighe hane y t the s n thear mes t arou ar he ond the on t ang be as ond mer m awond make ithe t t me bor the s alan ishe she s e be an winor n s this hit s s s halouche wis t ber n agheanghe ane hear s cou mof he d t ind he me the s I he d mare me soull pe chean You the here the fe br halanond t aco thino the,\n",
      "\n",
      "\n",
      "\n",
      "Whar than balakere ber be rand hes an s t t d halat the bes the chese t l s is ino le pour o me the s mer mis t d at there ang .\n",
      "An the for s the s and and whe te mind ar aner f alise hand this s the f ar t t be d anghe f inthe Re Bure and bure t and anond an thind t be hee f me sisere pofisher athit be s an iner an f the oule cer thothe this in de ineat the t be be har d t f ind there mat F t re he d thee mithen t thele the me e hather somere hace o :\n",
      "The bat t be oul the ave anor mis me t s I d the olle d the nd s t me he far thin ald a s s ar be alle beat be is alel loun d wabit he chan bout\n"
     ]
    }
   ],
   "source": [
    "idx = np.ones((1,1),dtype=np.int64)\n",
    "seq = model.generate(idx, 1000)[0].tolist()\n",
    "print(decode(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d87345-b031-4c94-befb-fd0bdcaaafc9",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "As you saw in the above results, you see the model is starting to identify patterns in data just after 1000 iterations, Yet it takes a lot of time to train a transformer in CPU and it's not possible to train a production grade model on CPU. Also this is a minimal version of the transformer which is identical to GPT2, but modern LLMS are much more sophisticated than this and also are trained on trillions of tokens on 1000s of GPU infrastructure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae1903-86d3-4c02-af51-079ac8ee70dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
